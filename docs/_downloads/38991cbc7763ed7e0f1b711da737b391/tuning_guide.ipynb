{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uc2e4 \ub54c\uc5d0\ub294 \n# https://tutorials.pytorch.kr/beginner/colab \ub97c \ucc38\uace0\ud558\uc138\uc694.\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performance Tuning Guide\n========================\n\n**Author**: [Szymon Migacz](https://github.com/szmigacz)\n\nPerformance Tuning Guide is a set of optimizations and best practices\nwhich can accelerate training and inference of deep learning models in\nPyTorch. Presented techniques often can be implemented by changing only\na few lines of code and can be applied to a wide range of deep learning\nmodels across all domains.\n\nGeneral optimizations\n---------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Enable asynchronous data loading and augmentation\n=================================================\n\n[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\nsupports asynchronous data loading and data augmentation in separate\nworker subprocesses. The default setting for `DataLoader` is\n`num_workers=0`, which means that the data loading is synchronous and\ndone in the main process. As a result the main training process has to\nwait for the data to be available to continue the execution.\n\nSetting `num_workers > 0` enables asynchronous data loading and overlap\nbetween the training and data loading. `num_workers` should be tuned\ndepending on the workload, CPU, GPU, and location of training data.\n\n`DataLoader` accepts `pin_memory` argument, which defaults to `False`.\nWhen using a GPU it\\'s better to set `pin_memory=True`, this instructs\n`DataLoader` to use pinned memory and enables faster and asynchronous\nmemory copy from the host to the GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Disable gradient calculation for validation or inference\n========================================================\n\nPyTorch saves intermediate buffers from all operations which involve\ntensors that require gradients. Typically gradients aren\\'t needed for\nvalidation or inference.\n[torch.no\\_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad)\ncontext manager can be applied to disable gradient calculation within a\nspecified block of code, this accelerates execution and reduces the\namount of required memory.\n[torch.no\\_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad)\ncan also be used as a function decorator.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Disable bias for convolutions directly followed by a batch norm\n===============================================================\n\n[torch.nn.Conv2d()](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\nhas `bias` parameter which defaults to `True` (the same is true for\n[Conv1d](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d)\nand\n[Conv3d](https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d)\n).\n\nIf a `nn.Conv2d` layer is directly followed by a `nn.BatchNorm2d` layer,\nthen the bias in the convolution is not needed, instead use\n`nn.Conv2d(..., bias=False, ....)`. Bias is not needed because in the\nfirst step `BatchNorm` subtracts the mean, which effectively cancels out\nthe effect of bias.\n\nThis is also applicable to 1d and 3d convolutions as long as `BatchNorm`\n(or other normalization layer) normalizes on the same dimension as\nconvolution\\'s bias.\n\nModels available from [torchvision](https://github.com/pytorch/vision)\nalready implement this optimization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use parameter.grad = None instead of model.zero\\_grad() or optimizer.zero\\_grad()\n=================================================================================\n\nInstead of calling:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.zero_grad()\n# or\noptimizer.zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "to zero out gradients, use the following method instead:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n    param.grad = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The second code snippet does not zero the memory of each individual\nparameter, also the subsequent backward pass uses assignment instead of\naddition to store gradients, this reduces the number of memory\noperations.\n\nSetting gradient to `None` has a slightly different numerical behavior\nthan setting it to zero, for more details refer to the\n[documentation](https://pytorch.org/docs/master/optim.html#torch.optim.Optimizer.zero_grad).\n\nAlternatively, starting from PyTorch 1.7, call `model` or\n`optimizer.zero_grad(set_to_none=True)`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fuse operations\n===============\n\nPointwise operations such as elementwise addition, multiplication, and\nmath functions like [sin()]{.title-ref}, [cos()]{.title-ref},\n[sigmoid()]{.title-ref}, etc., can be combined into a single kernel.\nThis fusion helps reduce memory access and kernel launch times.\nTypically, pointwise operations are memory-bound; PyTorch eager-mode\ninitiates a separate kernel for each operation, which involves loading\ndata from memory, executing the operation (often not the most\ntime-consuming step), and writing the results back to memory.\n\nBy using a fused operator, only one kernel is launched for multiple\npointwise operations, and data is loaded and stored just once. This\nefficiency is particularly beneficial for activation functions,\noptimizers, and custom RNN cells etc.\n\nPyTorch 2 introduces a compile-mode facilitated by TorchInductor, an\nunderlying compiler that automatically fuses kernels. TorchInductor\nextends its capabilities beyond simple element-wise operations, enabling\nadvanced fusion of eligible pointwise and reduction operations for\noptimized performance.\n\nIn the simplest case fusion can be enabled by applying\n[torch.compile](https://pytorch.org/docs/stable/generated/torch.compile.html)\ndecorator to the function definition, for example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@torch.compile\ndef gelu(x):\n    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Refer to [Introduction to\ntorch.compile](https://tutorials.pytorch.kr/intermediate/torch_compile_tutorial.html)\nfor more advanced use cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Enable channels\\_last memory format for computer vision models\n==============================================================\n\nPyTorch 1.5 introduced support for `channels_last` memory format for\nconvolutional networks. This format is meant to be used in conjunction\nwith [AMP](https://pytorch.org/docs/stable/amp.html) to further\naccelerate convolutional neural networks with [Tensor\nCores](https://www.nvidia.com/en-us/data-center/tensor-cores/).\n\nSupport for `channels_last` is experimental, but it\\'s expected to work\nfor standard computer vision models (e.g. ResNet-50, SSD). To convert\nmodels to `channels_last` format follow [Channels Last Memory Format\nTutorial](https://tutorials.pytorch.kr/intermediate/memory_format_tutorial.html).\nThe tutorial includes a section on [converting existing\nmodels](https://tutorials.pytorch.kr/intermediate/memory_format_tutorial.html#converting-existing-models).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checkpoint intermediate buffers\n===============================\n\nBuffer checkpointing is a technique to mitigate the memory capacity\nburden of model training. Instead of storing inputs of all layers to\ncompute upstream gradients in backward propagation, it stores the inputs\nof a few layers and the others are recomputed during backward pass. The\nreduced memory requirements enables increasing the batch size that can\nimprove utilization.\n\nCheckpointing targets should be selected carefully. The best is not to\nstore large layer outputs that have small re-computation cost. The\nexample target layers are activation functions (e.g. `ReLU`, `Sigmoid`,\n`Tanh`), up/down sampling and matrix-vector operations with small\naccumulation depth.\n\nPyTorch supports a native\n[torch.utils.checkpoint](https://pytorch.org/docs/stable/checkpoint.html)\nAPI to automatically perform checkpointing and recomputation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Disable debugging APIs\n======================\n\nMany PyTorch APIs are intended for debugging and should be disabled for\nregular training runs:\n\n-   anomaly detection:\n    [torch.autograd.detect\\_anomaly](https://pytorch.org/docs/stable/autograd.html#torch.autograd.detect_anomaly)\n    or\n    [torch.autograd.set\\_detect\\_anomaly(True)](https://pytorch.org/docs/stable/autograd.html#torch.autograd.set_detect_anomaly)\n-   profiler related:\n    [torch.autograd.profiler.emit\\_nvtx](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.emit_nvtx),\n    [torch.autograd.profiler.profile](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile)\n-   autograd `gradcheck`:\n    [torch.autograd.gradcheck](https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradcheck)\n    or\n    [torch.autograd.gradgradcheck](https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradgradcheck)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CPU specific optimizations\n==========================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilize Non-Uniform Memory Access (NUMA) Controls\n=================================================\n\nNUMA or non-uniform memory access is a memory layout design used in data\ncenter machines meant to take advantage of locality of memory in\nmulti-socket machines with multiple memory controllers and blocks.\nGenerally speaking, all deep learning workloads, training or inference,\nget better performance without accessing hardware resources across NUMA\nnodes. Thus, inference can be run with multiple instances, each instance\nruns on one socket, to raise throughput. For training tasks on single\nnode, distributed training is recommended to make each training process\nrun on one socket.\n\nIn general cases the following command executes a PyTorch script on\ncores on the Nth node only, and avoids cross-socket memory access to\nreduce memory access overhead.\n\n``` {.sourceCode .sh}\nnumactl --cpunodebind=N --membind=N python <pytorch_script>\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "More detailed descriptions can be found\n[here](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilize OpenMP\n==============\n\nOpenMP is utilized to bring better performance for parallel computation\ntasks. `OMP_NUM_THREADS` is the easiest switch that can be used to\naccelerate computations. It determines number of threads used for OpenMP\ncomputations. CPU affinity setting controls how workloads are\ndistributed over multiple cores. It affects communication overhead,\ncache line invalidation overhead, or page thrashing, thus proper setting\nof CPU affinity brings performance benefits. `GOMP_CPU_AFFINITY` or\n`KMP_AFFINITY` determines how to bind OpenMP\\* threads to physical\nprocessing units. Detailed information can be found\n[here](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the following command, PyTorch run the task on N OpenMP threads.\n\n``` {.sourceCode .sh}\nexport OMP_NUM_THREADS=N\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Typically, the following environment variables are used to set for CPU\naffinity with GNU OpenMP implementation. `OMP_PROC_BIND` specifies\nwhether threads may be moved between processors. Setting it to CLOSE\nkeeps OpenMP threads close to the primary thread in contiguous place\npartitions. `OMP_SCHEDULE` determines how OpenMP threads are scheduled.\n`GOMP_CPU_AFFINITY` binds threads to specific CPUs.\n\n``` {.sourceCode .sh}\nexport OMP_SCHEDULE=STATIC\nexport OMP_PROC_BIND=CLOSE\nexport GOMP_CPU_AFFINITY=\"N-M\"\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Intel OpenMP Runtime Library (`libiomp`)\n========================================\n\nBy default, PyTorch uses GNU OpenMP (GNU `libgomp`) for parallel\ncomputation. On Intel platforms, Intel OpenMP Runtime Library\n(`libiomp`) provides OpenMP API specification support. It sometimes\nbrings more performance benefits compared to `libgomp`. Utilizing\nenvironment variable `LD_PRELOAD` can switch OpenMP library to\n`libiomp`:\n\n``` {.sourceCode .sh}\nexport LD_PRELOAD=<path>/libiomp5.so:$LD_PRELOAD\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to CPU affinity settings in GNU OpenMP, environment variables\nare provided in `libiomp` to control CPU affinity settings.\n`KMP_AFFINITY` binds OpenMP threads to physical processing units.\n`KMP_BLOCKTIME` sets the time, in milliseconds, that a thread should\nwait, after completing the execution of a parallel region, before\nsleeping. In most cases, setting `KMP_BLOCKTIME` to 1 or 0 yields good\nperformances. The following commands show a common settings with Intel\nOpenMP Runtime Library.\n\n``` {.sourceCode .sh}\nexport KMP_AFFINITY=granularity=fine,compact,1,0\nexport KMP_BLOCKTIME=1\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Switch Memory allocator\n=======================\n\nFor deep learning workloads, `Jemalloc` or `TCMalloc` can get better\nperformance by reusing memory as much as possible than default `malloc`\nfunction. [Jemalloc](https://github.com/jemalloc/jemalloc) is a general\npurpose `malloc` implementation that emphasizes fragmentation avoidance\nand scalable concurrency support.\n[TCMalloc](https://google.github.io/tcmalloc/overview.html) also\nfeatures a couple of optimizations to speed up program executions. One\nof them is holding memory in caches to speed up access of commonly-used\nobjects. Holding such caches even after deallocation also helps avoid\ncostly system calls if such memory is later re-allocated. Use\nenvironment variable `LD_PRELOAD` to take advantage of one of them.\n\n``` {.sourceCode .sh}\nexport LD_PRELOAD=<jemalloc.so/tcmalloc.so>:$LD_PRELOAD\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use oneDNN Graph with TorchScript for inference\n===============================================\n\noneDNN Graph can significantly boost inference performance. It fuses\nsome compute-intensive operations such as convolution, matmul with their\nneighbor operations. In PyTorch 2.0, it is supported as a beta feature\nfor `Float32` & `BFloat16` data-types. oneDNN Graph receives the model's\ngraph and identifies candidates for operator-fusion with respect to the\nshape of the example input. A model should be JIT-traced using an\nexample input. Speed-up would then be observed after a couple of warm-up\niterations for inputs with the same shape as the example input. The\nexample code-snippets below are for resnet50, but they can very well be\nextended to use oneDNN Graph with custom models as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Only this extra line of code is required to use oneDNN Graph\ntorch.jit.enable_onednn_fusion(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the oneDNN Graph API requires just one extra line of code for\ninference with Float32. If you are using oneDNN Graph, please avoid\ncalling `torch.jit.optimize_for_inference`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sample input should be of the same shape as expected inputs\nsample_input = [torch.rand(32, 3, 224, 224)]\n# Using resnet50 from torchvision in this example for illustrative purposes,\n# but the line below can indeed be modified to use custom models as well.\nmodel = getattr(torchvision.models, \"resnet50\")().eval()\n# Tracing the model with example input\ntraced_model = torch.jit.trace(model, sample_input)\n# Invoking torch.jit.freeze\ntraced_model = torch.jit.freeze(traced_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once a model is JIT-traced with a sample input, it can then be used for\ninference after a couple of warm-up runs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n    # a couple of warm-up runs\n    traced_model(*sample_input)\n    traced_model(*sample_input)\n    # speedup would be observed after warm-up runs\n    traced_model(*sample_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While the JIT fuser for oneDNN Graph also supports inference with\n`BFloat16` datatype, performance benefit with oneDNN Graph is only\nexhibited by machines with AVX512\\_BF16 instruction set architecture\n(ISA). The following code snippets serves as an example of using\n`BFloat16` datatype for inference with oneDNN Graph:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# AMP for JIT mode is enabled by default, and is divergent with its eager mode counterpart\ntorch._C._jit_set_autocast_mode(False)\n\nwith torch.no_grad(), torch.cpu.amp.autocast(cache_enabled=False, dtype=torch.bfloat16):\n    # Conv-BatchNorm folding for CNN-based Vision Models should be done with ``torch.fx.experimental.optimization.fuse`` when AMP is used\n    import torch.fx.experimental.optimization as optimization\n    # Please note that optimization.fuse need not be called when AMP is not used\n    model = optimization.fuse(model)\n    model = torch.jit.trace(model, (example_input))\n    model = torch.jit.freeze(model)\n    # a couple of warm-up runs\n    model(example_input)\n    model(example_input)\n    # speedup would be observed in subsequent runs.\n    model(example_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train a model on CPU with PyTorch `DistributedDataParallel`(DDP) functionality\n==============================================================================\n\nFor small scale models or memory-bound models, such as DLRM, training on\nCPU is also a good choice. On a machine with multiple sockets,\ndistributed training brings a high-efficient hardware resource usage to\naccelerate the training process.\n[Torch-ccl](https://github.com/intel/torch-ccl), optimized with Intel(R)\n`oneCCL` (collective communications library) for efficient distributed\ndeep learning training implementing such collectives like `allreduce`,\n`allgather`, `alltoall`, implements PyTorch C10D `ProcessGroup` API and\ncan be dynamically loaded as external `ProcessGroup`. Upon optimizations\nimplemented in PyTorch DDP module, `torch-ccl` accelerates communication\noperations. Beside the optimizations made to communication kernels,\n`torch-ccl` also features simultaneous computation-communication\nfunctionality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GPU specific optimizations\n==========================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Enable cuDNN auto-tuner\n=======================\n\n[NVIDIA cuDNN](https://developer.nvidia.com/cudnn) supports many\nalgorithms to compute a convolution. Autotuner runs a short benchmark\nand selects the kernel with the best performance on a given hardware for\na given input size.\n\nFor convolutional networks (other types currently not supported), enable\ncuDNN autotuner before launching the training loop by setting:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   the auto-tuner decisions may be non-deterministic; different\n    algorithm may be selected for different runs. For more details see\n    [PyTorch:\n    Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html?highlight=determinism)\n-   in some rare cases, such as with highly variable input sizes, it\\'s\n    better to run convolutional networks with autotuner disabled to\n    avoid the overhead associated with algorithm selection for each\n    input size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Avoid unnecessary CPU-GPU synchronization\n=========================================\n\nAvoid unnecessary synchronizations, to let the CPU run ahead of the\naccelerator as much as possible to make sure that the accelerator work\nqueue contains many operations.\n\nWhen possible, avoid operations which require synchronizations, for\nexample:\n\n-   `print(cuda_tensor)`\n-   `cuda_tensor.item()`\n-   memory copies: `tensor.cuda()`, `cuda_tensor.cpu()` and equivalent\n    `tensor.to(device)` calls\n-   `cuda_tensor.nonzero()`\n-   python control flow which depends on results of operations performed\n    on CUDA tensors e.g. `if (cuda_tensor != 0).all()`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create tensors directly on the target device\n============================================\n\nInstead of calling `torch.rand(size).cuda()` to generate a random\ntensor, produce the output directly on the target device:\n`torch.rand(size, device='cuda')`.\n\nThis is applicable to all functions which create new tensors and accept\n`device` argument:\n[torch.rand()](https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand),\n[torch.zeros()](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros),\n[torch.full()](https://pytorch.org/docs/stable/generated/torch.full.html#torch.full)\nand similar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use mixed precision and AMP\n===========================\n\nMixed precision leverages [Tensor\nCores](https://www.nvidia.com/en-us/data-center/tensor-cores/) and\noffers up to 3x overall speedup on Volta and newer GPU architectures. To\nuse Tensor Cores AMP should be enabled and matrix/tensor dimensions\nshould satisfy requirements for calling kernels that use Tensor Cores.\n\nTo use Tensor Cores:\n\n-   set sizes to multiples of 8 (to map onto dimensions of Tensor Cores)\n    -   see [Deep Learning Performance\n        Documentation](https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance)\n        for more details and guidelines specific to layer type\n    -   if layer size is derived from other parameters rather than\n        fixed, it can still be explicitly padded e.g. vocabulary size in\n        NLP models\n-   enable AMP\n    -   Introduction to Mixed Precision Training and AMP:\n        [video](https://www.youtube.com/watch?v=jF4-_ZK_tyc&feature=youtu.be),\n        [slides](https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/dusan_stosic-training-neural-networks-with-tensor-cores.pdf)\n    -   native PyTorch AMP is available starting from PyTorch 1.6:\n        [documentation](https://pytorch.org/docs/stable/amp.html),\n        [examples](https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples),\n        [tutorial](https://tutorials.pytorch.kr/recipes/recipes/amp_recipe.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preallocate memory in case of variable input length\n===================================================\n\nModels for speech recognition or for NLP are often trained on input\ntensors with variable sequence length. Variable length can be\nproblematic for PyTorch caching allocator and can lead to reduced\nperformance or to unexpected out-of-memory errors. If a batch with a\nshort sequence length is followed by an another batch with longer\nsequence length, then PyTorch is forced to release intermediate buffers\nfrom previous iteration and to re-allocate new buffers. This process is\ntime consuming and causes fragmentation in the caching allocator which\nmay result in out-of-memory errors.\n\nA typical solution is to implement preallocation. It consists of the\nfollowing steps:\n\n1.  generate a (usually random) batch of inputs with maximum sequence\n    length (either corresponding to max length in the training dataset\n    or to some predefined threshold)\n2.  execute a forward and a backward pass with the generated batch, do\n    not execute an optimizer or a learning rate scheduler, this step\n    preallocates buffers of maximum size, which can be reused in\n    subsequent training iterations\n3.  zero out gradients\n4.  proceed to regular training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Distributed optimizations\n=========================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use efficient data-parallel backend\n===================================\n\nPyTorch has two ways to implement data-parallel training:\n\n-   [torch.nn.DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel)\n-   [torch.nn.parallel.DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel)\n\n`DistributedDataParallel` offers much better performance and scaling to\nmultiple-GPUs. For more information refer to the [relevant section of\nCUDA Best\nPractices](https://pytorch.org/docs/stable/notes/cuda.html#use-nn-parallel-distributeddataparallel-instead-of-multiprocessing-or-nn-dataparallel)\nfrom PyTorch documentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Skip unnecessary all-reduce if training with `DistributedDataParallel` and gradient accumulation\n================================================================================================\n\nBy default\n[torch.nn.parallel.DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel)\nexecutes gradient all-reduce after every backward pass to compute the\naverage gradient over all workers participating in the training. If\ntraining uses gradient accumulation over N steps, then all-reduce is not\nnecessary after every training step, it\\'s only required to perform\nall-reduce after the last call to backward, just before the execution of\nthe optimizer.\n\n`DistributedDataParallel` provides\n[no\\_sync()](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync)\ncontext manager which disables gradient all-reduce for particular\niteration. `no_sync()` should be applied to first `N-1` iterations of\ngradient accumulation, the last iteration should follow the default\nexecution and perform the required gradient all-reduce.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Match the order of layers in constructors and during the execution if using `DistributedDataParallel(find_unused_parameters=True)`\n==================================================================================================================================\n\n[torch.nn.parallel.DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel)\nwith `find_unused_parameters=True` uses the order of layers and\nparameters from model constructors to build buckets for\n`DistributedDataParallel` gradient all-reduce. `DistributedDataParallel`\noverlaps all-reduce with the backward pass. All-reduce for a particular\nbucket is asynchronously triggered only when all gradients for\nparameters in a given bucket are available.\n\nTo maximize the amount of overlap, the order in model constructors\nshould roughly match the order during the execution. If the order\ndoesn\\'t match, then all-reduce for the entire bucket waits for the\ngradient which is the last to arrive, this may reduce the overlap\nbetween backward pass and all-reduce, all-reduce may end up being\nexposed, which slows down the training.\n\n`DistributedDataParallel` with `find_unused_parameters=False` (which is\nthe default setting) relies on automatic bucket formation based on order\nof operations encountered during the backward pass. With\n`find_unused_parameters=False` it\\'s not necessary to reorder layers or\nparameters to achieve optimal performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load-balance workload in a distributed setting\n==============================================\n\nLoad imbalance typically may happen for models processing sequential\ndata (speech recognition, translation, language models etc.). If one\ndevice receives a batch of data with sequence length longer than\nsequence lengths for the remaining devices, then all devices wait for\nthe worker which finishes last. Backward pass functions as an implicit\nsynchronization point in a distributed setting with\n[DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel)\nbackend.\n\nThere are multiple ways to solve the load balancing problem. The core\nidea is to distribute workload over all workers as uniformly as possible\nwithin each global batch. For example Transformer solves imbalance by\nforming batches with approximately constant number of tokens (and\nvariable number of sequences in a batch), other models solve imbalance\nby bucketing samples with similar sequence length or even by sorting\ndataset by sequence length.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
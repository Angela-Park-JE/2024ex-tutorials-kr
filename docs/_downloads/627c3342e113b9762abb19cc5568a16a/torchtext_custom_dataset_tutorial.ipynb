{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uc2e4 \ub54c\uc5d0\ub294 \n# https://tutorials.pytorch.kr/beginner/colab \ub97c \ucc38\uace0\ud558\uc138\uc694.\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocess custom text dataset using Torchtext\n==============================================\n\n**Author**: [Anupam Sharma](https://anp-scp.github.io/)\n\nThis tutorial illustrates the usage of torchtext on a dataset that is\nnot built-in. In the tutorial, we will preprocess a dataset that can be\nfurther utilized to train a sequence-to-sequence model for machine\ntranslation (something like, in this tutorial: [Sequence to Sequence\nLearning with Neural\nNetworks](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb))\nbut without using legacy version of torchtext.\n\nIn this tutorial, we will learn how to:\n\n-   Read a dataset\n-   Tokenize sentence\n-   Apply transforms to sentence\n-   Perform bucket batching\n\nLet us assume that we need to prepare a dataset to train a model that\ncan perform English to German translation. We will use a tab-delimited\nGerman - English sentence pairs provided by the [Tatoeba\nProject](https://tatoeba.org/en) which can be downloaded from [this\nlink](https://www.manythings.org/anki/deu-eng.zip).\n\nSentence pairs for other languages can be found in [this\nlink](https://www.manythings.org/anki/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup\n=====\n\nFirst, download the dataset, extract the zip, and note the path to the\nfile [deu.txt]{.title-ref}.\n\nEnsure that following packages are installed:\n\n-   [Torchdata 0.6.0](https://pytorch.org/data/beta/index.html)\n    ([Installation instructions](https://github.com/pytorch/data))\n-   [Torchtext 0.15.0](https://pytorch.org/text/stable/index.html)\n    ([Installation instructions](https://github.com/pytorch/text))\n-   [Spacy](https://spacy.io/usage)\n\nHere, we are using [Spacy]{.title-ref} to tokenize text. In simple words\ntokenization means to convert a sentence to list of words. Spacy is a\npython package used for various Natural Language Processing (NLP) tasks.\n\nDownload the English and German models from Spacy as shown below:\n\n``` {.sourceCode .shell}\npython -m spacy download en_core_web_sm\npython -m spacy download de_core_news_sm\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us start by importing required modules:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torchdata.datapipes as dp\nimport torchtext.transforms as T\nimport spacy\nfrom torchtext.vocab import build_vocab_from_iterator\neng = spacy.load(\"en_core_web_sm\") # Load the English model to tokenize English text\nde = spacy.load(\"de_core_news_sm\") # Load the German model to tokenize German text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will load the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "FILE_PATH = 'data/deu.txt'\ndata_pipe = dp.iter.IterableWrapper([FILE_PATH])\ndata_pipe = dp.iter.FileOpener(data_pipe, mode='rb')\ndata_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\\t', as_tuple=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above code block, we are doing following things:\n\n1.  At line 2, we are creating an iterable of filenames\n2.  At line 3, we pass the iterable to [FileOpener]{.title-ref} which\n    then opens the file in read mode\n3.  At line 4, we call a function to parse the file, which again returns\n    an iterable of tuples representing each rows of the tab-delimited\n    file\n\nDataPipes can be thought of something like a dataset object, on which we\ncan perform various operations. Check [this\ntutorial](https://pytorch.org/data/beta/dp_tutorial.html) for more\ndetails on DataPipes.\n\nWe can verify if the iterable has the pair of sentences as shown below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for sample in data_pipe:\n    print(sample)\n    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we also have attribution details along with pair of sentences.\nWe will write a small function to remove the attribution details:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def removeAttribution(row):\n    \"\"\"\n    Function to keep the first two elements in a tuple\n    \"\"\"\n    return row[:2]\ndata_pipe = data_pipe.map(removeAttribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The [map]{.title-ref} function at line 6 in above code block can be used\nto apply some function on each elements of [data\\_pipe]{.title-ref}.\nNow, we can verify that the [data\\_pipe]{.title-ref} only contains pair\nof sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for sample in data_pipe:\n    print(sample)\n    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let us define few functions to perform tokenization:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def engTokenize(text):\n    \"\"\"\n    Tokenize an English text and return a list of tokens\n    \"\"\"\n    return [token.text for token in eng.tokenizer(text)]\n\ndef deTokenize(text):\n    \"\"\"\n    Tokenize a German text and return a list of tokens\n    \"\"\"\n    return [token.text for token in de.tokenizer(text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above function accepts a text and returns a list of words as shown\nbelow:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(engTokenize(\"Have a good day!!!\"))\nprint(deTokenize(\"Haben Sie einen guten Tag!!!\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Building the vocabulary\n=======================\n\nLet us consider an English sentence as the source and a German sentence\nas the target.\n\nVocabulary can be considered as the set of unique words we have in the\ndataset. We will build vocabulary for both our source and target now.\n\nLet us define a function to get tokens from elements of tuples in the\niterator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def getTokens(data_iter, place):\n    \"\"\"\n    Function to yield tokens from an iterator. Since, our iterator contains\n    tuple of sentences (source and target), `place` parameters defines for which\n    index to return the tokens for. `place=0` for source and `place=1` for target\n    \"\"\"\n    for english, german in data_iter:\n        if place == 0:\n            yield engTokenize(english)\n        else:\n            yield deTokenize(german)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we will build vocabulary for source:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "source_vocab = build_vocab_from_iterator(\n    getTokens(data_pipe,0),\n    min_freq=2,\n    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n    special_first=True\n)\nsource_vocab.set_default_index(source_vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code above, builds the vocabulary from the iterator. In the above\ncode block:\n\n-   At line 2, we call the [getTokens()]{.title-ref} function with\n    [place=0]{.title-ref} as we need vocabulary for source sentences.\n-   At line 3, we set [min\\_freq=2]{.title-ref}. This means, the\n    function will skip those words that occurs less than 2 times.\n-   At line 4, we specify some special tokens:\n    -   [\\<sos\\>]{.title-ref} for start of sentence\n    -   [\\<eos\\>]{.title-ref} for end of sentence\n    -   [\\<unk\\>]{.title-ref} for unknown words. An example of unknown\n        word is the one skipped because of [min\\_freq=2]{.title-ref}.\n    -   [\\<pad\\>]{.title-ref} is the padding token. While training, a\n        model we mostly train in batches. In a batch, there can be\n        sentences of different length. So, we pad the shorter sentences\n        with [\\<pad\\>]{.title-ref} token to make length of all sequences\n        in the batch equal.\n-   At line 5, we set [special\\_first=True]{.title-ref}. Which means\n    [\\<pad\\>]{.title-ref} will get index 0, [\\<sos\\>]{.title-ref} index\n    1, [\\<eos\\>]{.title-ref} index 2, and \\<unk\\> will get index 3 in\n    the vocabulary.\n-   At line 7, we set default index as index of [\\<unk\\>]{.title-ref}.\n    That means if some word is not in vocabulary, we will use\n    [\\<unk\\>]{.title-ref} instead of that unknown word.\n\nSimilarly, we will build vocabulary for target sentences:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target_vocab = build_vocab_from_iterator(\n    getTokens(data_pipe,1),\n    min_freq=2,\n    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n    special_first=True\n)\ntarget_vocab.set_default_index(target_vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the example above shows how can we add special tokens to our\nvocabulary. The special tokens may change based on the requirements.\n\nNow, we can verify that special tokens are placed at the beginning and\nthen other words. In the below code,\n[source\\_vocab.get\\_itos()]{.title-ref} returns a list with tokens at\nindex based on vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(source_vocab.get_itos()[:9])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Numericalize sentences using vocabulary\n=======================================\n\nAfter building the vocabulary, we need to convert our sentences to\ncorresponding indices. Let us define some functions for this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def getTransform(vocab):\n    \"\"\"\n    Create transforms based on given vocabulary. The returned transform is applied to sequence\n    of tokens.\n    \"\"\"\n    text_tranform = T.Sequential(\n        ## converts the sentences to indices based on given vocabulary\n        T.VocabTransform(vocab=vocab),\n        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n        # 1 as seen in previous section\n        T.AddToken(1, begin=True),\n        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n        # 2 as seen in previous section\n        T.AddToken(2, begin=False)\n    )\n    return text_tranform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let us see how to use the above function. The function returns an\nobject of [Transforms]{.title-ref} which we will use on our sentence.\nLet us take a random sentence and check how the transform works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "temp_list = list(data_pipe)\nsome_sentence = temp_list[798][0]\nprint(\"Some sentence=\", end=\"\")\nprint(some_sentence)\ntransformed_sentence = getTransform(source_vocab)(engTokenize(some_sentence))\nprint(\"Transformed sentence=\", end=\"\")\nprint(transformed_sentence)\nindex_to_string = source_vocab.get_itos()\nfor index in transformed_sentence:\n    print(index_to_string[index], end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above code,:\n\n-   At line 2, we take a source sentence from list that we created from\n    [data\\_pipe]{.title-ref} at line 1\n-   At line 5, we get a transform based on a source vocabulary and apply\n    it to a tokenized sentence. Note that transforms take list of words\n    and not a sentence.\n-   At line 8, we get the mapping of index to string and then use it get\n    the transformed sentence\n\nNow we will use DataPipe functions to apply transform to all our\nsentences. Let us define some more functions for this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def applyTransform(sequence_pair):\n    \"\"\"\n    Apply transforms to sequence of tokens in a sequence pair\n    \"\"\"\n\n    return (\n        getTransform(source_vocab)(engTokenize(sequence_pair[0])),\n        getTransform(target_vocab)(deTokenize(sequence_pair[1]))\n    )\ndata_pipe = data_pipe.map(applyTransform) ## Apply the function to each element in the iterator\ntemp_list = list(data_pipe)\nprint(temp_list[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make batches (with bucket batch)\n================================\n\nGenerally, we train models in batches. While working for sequence to\nsequence models, it is recommended to keep the length of sequences in a\nbatch similar. For that we will use [bucketbatch]{.title-ref} function\nof [data\\_pipe]{.title-ref}.\n\nLet us define some functions that will be used by the\n[bucketbatch]{.title-ref} function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def sortBucket(bucket):\n    \"\"\"\n    Function to sort a given bucket. Here, we want to sort based on the length of\n    source and target sequence.\n    \"\"\"\n    return sorted(bucket, key=lambda x: (len(x[0]), len(x[1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we will apply the [bucketbatch]{.title-ref} function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_pipe = data_pipe.bucketbatch(\n    batch_size = 4, batch_num=5,  bucket_num=1,\n    use_in_batch_shuffle=False, sort_key=sortBucket\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above code block:\n\n> -   We keep batch size = 4.\n> -   [batch\\_num]{.title-ref} is the number of batches to keep in a\n>     bucket\n> -   [bucket\\_num]{.title-ref} is the number of buckets to keep in a\n>     pool for shuffling\n> -   [sort\\_key]{.title-ref} specifies the function that takes a bucket\n>     and sorts it\n\nNow, let us consider a batch of source sentences as [X]{.title-ref} and\na batch of target sentences as [y]{.title-ref}. Generally, while\ntraining a model, we predict on a batch of [X]{.title-ref} and compare\nthe result with [y]{.title-ref}. But, a batch in our\n[data\\_pipe]{.title-ref} is of the form \\`\\[(X\\_1,y\\_1), (X\\_2,y\\_2),\n(X\\_3,y\\_3), (X\\_4,y\\_4)\\]\\`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(list(data_pipe)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, we will now convert them into the form: [((X\\_1,X\\_2,X\\_3,X\\_4),\n(y\\_1,y\\_2,y\\_3,y\\_4))]{.title-ref}. For this we will write a small\nfunction:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def separateSourceTarget(sequence_pairs):\n    \"\"\"\n    input of form: `[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]`\n    output of form: `((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))`\n    \"\"\"\n    sources,targets = zip(*sequence_pairs)\n    return sources,targets\n\n## Apply the function to each element in the iterator\ndata_pipe = data_pipe.map(separateSourceTarget)\nprint(list(data_pipe)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we have the data as desired.\n\nPadding\n=======\n\nAs discussed earlier while building vocabulary, we need to pad shorter\nsentences in a batch to make all the sequences in a batch of equal\nlength. We can perform padding as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def applyPadding(pair_of_sequences):\n    \"\"\"\n    Convert sequences to tensors and apply padding\n    \"\"\"\n    return (T.ToTensor(0)(list(pair_of_sequences[0])), T.ToTensor(0)(list(pair_of_sequences[1])))\n## `T.ToTensor(0)` returns a transform that converts the sequence to `torch.tensor` and also applies\n# padding. Here, `0` is passed to the constructor to specify the index of the `<pad>` token in the\n# vocabulary.\ndata_pipe = data_pipe.map(applyPadding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can use the index to string mapping to see how the sequence\nwould look with tokens instead of indices:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "source_index_to_string = source_vocab.get_itos()\ntarget_index_to_string = target_vocab.get_itos()\n\ndef showSomeTransformedSentences(data_pipe):\n    \"\"\"\n    Function to show how the sentences look like after applying all transforms.\n    Here we try to print actual words instead of corresponding index\n    \"\"\"\n    for sources,targets in data_pipe:\n        if sources[0][-1] != 0:\n            continue # Just to visualize padding of shorter sentences\n        for i in range(4):\n            source = \"\"\n            for token in sources[i]:\n                source += \" \" + source_index_to_string[token]\n            target = \"\"\n            for token in targets[i]:\n                target += \" \" + target_index_to_string[token]\n            print(f\"Source: {source}\")\n            print(f\"Traget: {target}\")\n        break\n\nshowSomeTransformedSentences(data_pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above output we can observe that the shorter sentences are padded\nwith [\\<pad\\>]{.title-ref}. Now, we can use [data\\_pipe]{.title-ref}\nwhile writing our training function.\n\nSome parts of this tutorial was inspired from [this\narticle](https://medium.com/@bitdribble/migrate-torchtext-to-the-new-0-9-0-api-1ff1472b5d71).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}